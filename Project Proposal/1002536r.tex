\documentclass{acm_proc_article-sp}

\title{Coupling the distributed Large Eddy Simulation and Weather Research and
Forecasting model using OASIS3-MCT and MPI}

\numberofauthors{1}

\author{
    \alignauthor
    Gordon Reid\\
    \affaddr{School of Computing Science}\\
    \affaddr{University of Glasgow}\\
    \email{1002536r@student.gla.ac.uk}
}

\let\underscore\_
\renewcommand{\_}{\underscore\hspace{0pt}}

\begin{document}

\maketitle

\begin{abstract}

Scientists studying the geosciences have created software which models different
aspects of our planet's ecosystem, for example ocean, wind, land and atmosphere.
On their own, each model is useful however for higher accuracy, and to model
more complex aspects of our climate, these models need to be coupled together.
The project proposed involves creating a distributed version of the Large Eddy
Simulator (LES) then coupling the LES with the Weather Research and Forecasting
model (WRF). The coupled system should scale to large computing clusters.
Ideally the coupling would also be generic such that the OpenCL-accelerated LES
could be used to allow greater performance on heterogeneous systems.

\end{abstract}

\section*{Introduction}

Our planet's ecosystem is highly complex. Geoscientists that are interested in
modelling this ecosystem have created numerous systems which each model one
aspect, for instance a Large Eddy Simulator (LES) to study turbulent air flows
\cite{Nakayama2011,Nakayama2012} and the Weather Research and Forecasting Model
(WRF) for mesoscale weather prediction. For the last decade or so, interest has
evolved from individual models to combining models \cite{Michalakes2010}. With
this trend, a number of model coupling frameworks have been created to support
the desire for co-simulation. These allow a single application to make use of
multiple models without significant changes to the individual models code bases.
Models communicate with each other via data exchanges at each timestep.

The proposed work for the project involves two distinct stages. The first
requires the modification of existing LES code to create a distributed version
using the Message Passing Interface (MPI) for use on computer clusters. An
additional aim is to allow use of the OpenCL-accelerated LES
\cite{Vanderbauwhede2014} in the distributed system to improve the model's
throughput further. The second stage involves the coupling of the distributed
LES with the WRF using a model coupling framework. There is a potential third
stage which involves comparing the coupling to the coupling using the Glasgow
Model Coupling Framework (GCMF) \cite{Vanderbauwhede2014}.

A number of modeling frameworks have been looked at, including: the Model
Coupling Toolkit (MCT) \cite{Larson2005,Jacob2005}; OASIS3-MCT
\cite{Valcke,Valcke2013}; the Earth System Modeling Framework (ESMF)
\cite{Ramework2004}; and OpenPALM \cite{Piacentini2011}. After comparing these
modeling frameworks, OASIS3-MCT was chosen given its continued development and
simple coupling mechanism.

The remainder of the document discusses the research problem in detail before
going onto an in-depth literature review for the state of the art in distributed
and coupled applications. The proposed approach and work plan for the project is
then given to conclude the proposal.

\section*{Statement of Problem}

Individuals models for ocean, land, atmosphere, etc are useful in ther own right
however geoscientists are now wanting to model more complex scenarios that
require collaboration of multiple models. The collaboration can also lead to
more accurate results since additional effects of other parts of the system can
be added to the model in question e.g. merging WRF and LES led to greater wind
velocity being predicted compared with running WRF on its own
\cite{Kinbara2010,Nakayama1998}.

Currently, WRF and LES are independent models and LES has two variants: the
original single threaded code and the OpenCL-accelerated variant
\cite{Vanderbauwhede2014}. Vanderbauwhede and Takemi \cite{Vanderbauwhede2013}
have also investigated the benefits of GPU accelerating WRF and found this to be
``feasible and worthwhile''.

The problem to be addressed is two-fold: creating a MPI variant of LES to allow
LES to run on multiple nodes in a computer cluster and then coupling this
variant of LES with WRF. The MPI variant of the LES will increase the
performance of the simulator since it will allow it to make use of a multi-node
distributed memory system such as a Beowulf cluster. The coupling of MPI LES
with WRF will allow create a system that benefits from scalable performance and
high accuracy results.

\section*{Literature Review}

Previous work falls under two related but independent areas: distributed
computing using MPI for data sharing and model coupling. Model couplers
generally use MPI for cross-model communication however this is generally hidden
to the user under an API offered by the model coupling framework. MPI specific
work will be discussed followed by a section dedicated to related work on model
coupling.

\subsection*{Message Passing Interface}

Message Passing Interface (MPI) is a recognised standard for sharing data
between processes on parallel systems and supports message passing between
distinct computing nodes in a connected cluster. The standard creates a common
interface that allows anybody to create their own implementation and ensures
that different implementations of the same version of the standard can be
interchanged without requiring user code changes. This has led MPI to be a
\textit{de facto} standard for inter-process communication, especially across
multiple nodes.

\subsubsection*{Multithreaded MPI Communication}

In addition to computer systems making use of multiple machines, each individual
node now has multiple CPU cores and multiple CPU sockets. This poses an
additional problem for application developers: how to enable the use of MPI
inside a multithreaded application? Multiple threads and MPI individually bring
their own complexity however there is an additional challenge when considering
how multiple threads make MPI calls. The MPI standard defines four levels of
thread support: \textbf{MPI\_THREAD\_SINGLE} requiring user code to be single
threaded; \textbf{MPI\_THREAD\_FUNNELED} allowing user code to be multithreaded
but one thread makes MPI calls; \textbf{MPI\_THREAD\_SERIALIZED} allowing user
code to be multithreaded but MPI library calls are serialised and
\textbf{MPI\_THREAD\_MULTIPLE} where threads can make MPI calls without
concerning themselves about the other threads. Thakur and Gropp
\cite{Thakur2009} and D\'{o}zsa and Kuma et al.\ \cite{Kumar} have discussed the
challenges of code correctness and performance of multithreaded MPI
communication however they only focus on the \textbf{MPI\_THREAD\_MULTIPLE}
case. This limitation isn't necessarily significant given that this is the most
general case with \textbf{MPI\_THREAD\_FUNNELLED} and
\textbf{MPI\_THREAD\_SERIALIZED} being restricted cases of multithread MPI
programs.

D\'{o}zsa and Kuma et al.\ \cite{Kumar} use a multichannel-enabled network
hardware, granular locking, atomic operations, and concurrency-aware message
queues to demonstrate that multithreaded applications can have a message passing
rate that scales with multiple threads. The authors send zero-byte messages to
measure the number of messages that can be sent per second as the number of
threads grows. Performance is still worse than the Deep Computing Message
Framework (DCMF) that is a lower level messaging library and has a message rate
that scales almost linearly with the number of threads. The author's also
haven't considered latency of messages or raw throughput since there may be an
effect of the size of messages in a threaded environment and zero-byte messages
are unrealistic. The authors have only investigated message rate for a small
range of threads (1 to 4) however there can be single nodes with up to 64 cores
(16 core CPUs on a quad-socket motherboard) which is where any
multithreading-related performance problems would come to light.

Thakur and Gropp \cite{Thakur2009} create and use a number of performance
benchmarks using scenarios claimed to be close to typical applications. They
test the cost of thread safety, the ability for concurrent progress to be made,
and the possibilities for computation overlap. The authors first test involves a
simple ping-pong latency test of a single threaded application that calls
\textbf{MPI\_Init\_thread} with \textbf{MPI\_THREAD\_MULTIPLE}. The test shows
negligible latency on some MPI implementations. The validity of this test can be
question because, even though the MPI library is told to expect multiple threads
making calls simultaneously, this doesn't happen and thus the effect of locks
etc may not be being appropriately exercised to determine the overhead. The
authors second test investigated cumulative bandwidth of MPICH2 and Open MPI
implementations, comparing each libraries performance where parallelism is
achieved via processes or threads. For the Linux cluster, performance in all
cases saturates the 1Gbit/s link between nodes and thus no conclusions can be
made. For the Sun and IBM cases, threading is shown to dramatically decrease
throughput (over 50\%) in the threaded case compared to the process parallel
case. The bandwidth in Sun's multithreaded case was less than the saturated
bandwidth multithreaded case for the Linux cluster so this appears to show that
Sun can significantly improve the performance of their MPI implementation.

The other tests overall show how MPICH2 and Open MPI have fairly good
performance however the authors note that, due to the Gigabit Ethernet
interconnect, the overheads are generally masked since the ethernet connection
is saturated in all cases, as shown by the cumulative bandwidth test. The paper
has a more comprehensive look into the performance differences of MPI libraries
and shows that significant work is required to minimise the overhead caused by
multithreaded MPI applications. The authors also make their benchmark test suite
available online for others to repeat their tests and see how different hardware
setups react and how later versions of MPI libraries improve.

\subsection*{Model Couplers}

Applications currently exist that model a single aspect of our Earth's climate.
Each model is useful in their own right however more accurate and detailed
analysis can be conducted by making use of multiple models. For instance, the
effect ocean and land temperatures have on airflows could be investigated. This
isn't viable by using the models independently so there is significant
motivation to construct an efficient method of allowing applications to
interact.

There are three ways applications can be coupled together \cite{Thevenin}. The
first involves merging the two code bases. This solution is very efficient and
portable as a single process is compiled. Data is shared via memory exchanges
which are very lightweight however the process of merging is very complicated
and the two code bases are no longer independent making continued development of
each model difficult. Merging also causes problems for parallelism since each
model's parallel abilities are dictated by the overarching code that controls
both models. For these reasons this solution is not an appropriate way to couple
models.

The second solution \cite{Thevenin} is to use a communication library directly,
such as MPI. This allows each model to be kept independent with the only code
changes involved being related to the communication between each model itself.
The solution still suffers from many problems. The coupling code isn't generic
since each model needs to know exactly what model it is sending data to. Also,
since the MPI API is fairly low level, knowledge of parallel computing
techniques is required for efficient coupling. The problems of complexity and
genericness increase if there are many models involved in the coupling with
complicated exchanges required. Using MPI for data exchanges is definitely a
viable way forward however a higher level solution is required to make coupling
many models viable.

The third solution \cite{Thevenin} follows on from the second. Instead of using
an MPI library directly, a coupling library should be employed to extract the
low level complexities of coupling. A coupling library is a separate piece of
software that acts as the ``middle man'' between models and facilitates data
exchanges between models using a generic interface. Coupling libraries can also
provide tools for interpolation such as re-gridding if there is a difference
between data representations between communicating models. Some coupling
libraries also supply tools for performance analysis with graphical
representations of runtime execution characteristics. There are many examples of
coupling libraries in use, for example: the Model Coupling Toolkit (MCT)
\cite{Larson2005,Jacob2005}; OASIS3-MCT \cite{Valcke,Valcke2013}; the Earth
System Modeling Framework (ESMF) \cite{Ramework2004}; and OpenPALM
\cite{Piacentini2011}.

There are two main types of coupling: static and dynamic
\cite{Thevenin,Piacentini2011}. In static coupling, all of the coupled
components have to start simultaneously at the beginning of the simulation. This
means any memory and CPU resources required by each model are allocated and
locked until the end of the simulation, even if a model doesn't start using the
resources until midway through the simulation. Dynamic coupling allows each
coupled component to start and end as required, freeing up resources when not in
use. Instead, resources are managed by the coupler. Generally there is a fixed
amount of resources available to the coupler at the beginning of the simulation
however the coupler is free to dynamically manage how the resources are used to
meet the requirements of the currently running components.

Coupling parallel models has a problem that is generic to all models and model
coupling frameworks. This is known as the ``Parallel Coupling Problem'' or
``M-by-N problem''. Larson et al.\ \cite{Larson2005,Jacob2005} discuss this in
detail with respect to the creation of the Model Coupling Toolkit....... DISCUSS
THIS PROBLEM

Each of the listed model couplers have many similarities however they also have
a number of differences in terms of how they couple models and the tools
offered. A comparison of model couplers is given below.

\subsubsection*{Comparison of Model Couplers}

The Model Coupling Toolkit (MCT) \cite{Larson2005,Jacob2005} is a simple
coupling framework which has been very successful having been used in the
Coupled Ocean-Atmosphere-Wave-Sediment Transport (COAWST) modelling system,
COAMPS for coastal atmosphere simulation, and WRF. The toolkit also forms the
underlying code for the coupler in the Community Climate System Model (CCSM).
The project is suffering from bitrot, with the last commit to the MCT repository
in December 2012. However, the OASIS team have integrated MCT code with their
latest framework, OASIS3-MC,T so the framework is still in active use. For these
reasons MCT will not be considered in its own right, instead considered with
respect to OASIS3-MCT which is in active development.

ESMF

OASIS3-MCT

OpenPALM

\section*{Proposed Approach}

State how you propose to solve the software development problem. Show that your
approach is feasible, but identify any risks.

\section*{Work Plan}

The project has a number of distinct stages with deliverables at the end of each
stage. The following major stages have been identified with the estimated
delivery date given in bold:

\begin{description}
	\item[25th November] Create distributed LES using MPI
	\item[15th December] Couple LES and WRF using OASIS3-MCT
	\item[3rd February] Evaluate performance of different node count and node
    configurations for:
    \begin{itemize}
        \item MPI LES (using multiple MPI libraries if available)
        \item Coupled System
    \end{itemize}
	\item[10th March] Compare coupling framework to GCMF
	\item[31st March] Evaluate differences in performance between OASIS3-MCT
    coupling and GCMF Coupling
    \item[24th April Estimated] Final deliverable in the form of a paper/poster
    describing work completed.
\end{description}

\bibliographystyle{acm}
\bibliography{1002536r}

\end{document}

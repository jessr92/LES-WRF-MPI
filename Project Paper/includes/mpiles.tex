The \textit{de facto} standard for parallel communication in a distributed
memory system is the Message Passing Interface. As such, it was chosen to
implement a parallelised LES to act as a baseline for GMCF performance as part
of the shared-memory performance evaluation. It can also be used in the future
to compare GMCF's distributed memory performance when support for that is added.

In addition to acting as a performance baseline, the MPI implementation is the
first distributed-memory parallel implementation of this particular simulator.
This will allow researchers to simulate at a higher resolution or over larger
areas than the original single threaded version with more flexibility than the
OpenCL-accelerated implementation.

\subsection{Required communication}

The LES works over a three dimensional area. When parallelising the LES, the
processes are logically organised into a two dimensional virtual topology over
the geographical area under simulation. Each process computes over their own
slice of the three dimensional area.

The LES has nearly one hundred arrays that represent a different aspect of the
area under simulation. Some of these arrays will need to communicate with the
other processes in various fashions.

Processes will need to distribute and gather whole arrays. This occurs at the
beginning and end of the LES's execution. At the beginning, a single process
acting as the master process will read in the entire input data and distribute
the result to the other processes. At the end, the master process will gather
the results of the simulation to be written out to disk in the netCDF format.

Another kind of required communication is the exchange of halos, or array
boundaries. To calculate a given index in an array, the surrounding values are
used as part of the calculation. Near the boundaries of each process's array,
some of the surrounding values will be present in the memory space of a
neighbouring process. A halo exchange is the copying of array boundary values
between neighbouring processes. A single process may have to communicate with up
to eight processes: four for the top, bottom, left, and right boundaries; and a
further four for the remaining corner values.

The LES has a second type of array boundary communication to manage the side
flows. In the original LES, the left edge of some arrays is copied to the right
edge and vice versa. This is done to have the simulated area `wrap around'
itself in the direction perpendicular to the direction of the inflow. This is
not done for the top and bottom edges since they act as the outflow and inflow
respectively which require no communication between processes.

All of the communication discussed so far is able to use the standard
MPI\_Send/Recv API (and their non-blocking counterparts).

The final communication required is global reductions. During the solving of the
Poisson equation, the sum of scalar values is required. There are also global
minimum and global maximum values for scalar variables needed at the beginning
of each time step. This is a form of all-to-all communication since the value in
each process is needed to calculate the final value which is then given to all
processes. This can be very expensive so a smart algorithm for calculating the
global reductions is required. The MPI standard has a specific API call,
MPI\_AllReduce, allowing MPI implementations to devise their own algorithm
without restrictions.

\subsection{Implementation techniques}

With the LES algorithms themselves being developed separately, it was highly
desirable to parallelise the LES by making as few changes to the existing code
as possible. To facilitate this, the communication code was strongly separated
into distinct, generic modules. This meant code changes to the LES were minimal,
with most of the changes being the addition of calls to the Fortran module that
had the communication logic contained within it. In addition to keeping the
communication code separated from the LES code, the communication code itself
was made generic such that GMCF could be used in-place of MPI easily, allowing
them to share the same pre- and post-communication code.

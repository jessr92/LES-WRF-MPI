The GMCF performance evaluation discussed in Section~\ref{sec:GMCFLESEval} was
the first ever to be conducted on the framework. The results discussed in the
evaluation were the final best results obtained after a number of changes were
made to the framework to improve performance.

Also, during development of the GMCF parallelised LES, an unusual bug was
encountered caused by Fortran and C++ interoperability through POSIX threads.
The bug is not framework specific but will be discussed in
Section~\ref{sec:fortrancppinteroperability} as it is applicable to any
application using POSIX threads to call Fortran subroutines.

\subsection{Performance improvements}

\begin{figure}
    \includegraphics[width=0.5\textwidth]
    {graphs/GMCF-before-after-fixed-area.png}
    \caption{GMCF Before and After}
    \label{fig:gmcfbeforeandafter}
\end{figure}

Figure~\ref{fig:gmcfbeforeandafter} shows the original GMCF performance results
alongside the best results for a fixed area run. As is clear from the graph,
GMCF was suffering from scalability issues, with the runtime increasing above
ten threads and causing a bathtub curve. A number of framework changes including
a dedicated method of calculating global reductions, spin locks, and thread
pinning were investigated and were ultimately implemented to improve
performance. Overall these changes were responsible for making GMCF performance
scalable as the number of threads grows and reducing GMCF performance to nearly
one sixth for a fixed area problem.

\subsubsection{Global Reduction}

The first task in the performance investigation was to look for aspects of
communication that were causing the bathtub curve. The performance degradation
at ever increasing levels of threads pointed towards global communication. Only
one kind of global communication occurs in each LES time step, global
reductions. The maximum, minimum, and sum of a small number of scalars is
required. One of those reductions, a global sum, is contained within an inner
loop and is thus calculated up to fifty times per main loop iteration.
Temporarily disabling this global sum caused the runtime graph to flatten out
without significant runtime degradation at higher threading levels. Runtime was
now around 150 seconds rather than nearly 320 seconds at 64 threads which was a
significant improvement but still nearly six times slower than MPI.

With the cause of the bathtub curve found, a number of different ways of
calculating a global reduction were investigated. The original method involved
message passing in the same way halo exchanges and other arrays were passed. The
overhead of sending packets for a single scalar were clearly significant. Also,
a single thread was the recipient of all of the scalars, one from each thread.
That same thread then calculated the global reduction, then sent the result to
every other threads. This caused a single thread to have an ever increasing
amount of work to do compared to other threads, a problem that became worse as
the total thread count grew. In a micro benchmark, GMCF was 100x slower than MPI
when calculating a global sum over 4 threads and 1000x slower over 64 threads.

The solution was a specific subroutine embedded within the GMCF framework that
did not use messages but instead a global variable. Rather than have a single
thread do the heavy lifting of receiving, calculating, and sending results, each
thread accessed the global variable and did a partial calculation. Once all
threads had done the partial calculation, all threads can then simultaneously
read the global variable to get the final result. Many methods of this were
investigated with the final implementation involving a POSIX thread spinlock and
busy waiting. This combination of spinlock and busy waiting offered a 10x
improvement over a more traditional mutex and condition variable combination.
The performance of the micro benchmarks were within 10\% of MPI.

When the global reductions in LES were re-enabled to use the new method of
calculation, the global reduction overhead became negligible.

OpenMPI's method of global reduction uses a ``recursive doubling algorithm''.
This involves a tree-like reduction that in a work depth that is logarithmic
with the number of communicating processes. GMCFs new method has a work depth
that is linear with the number of threads. For tens of processes, MPIs tree-like
algorithm does not improve performance of the reductions however it may be
beneficial as the number of processes grows into the hundreds and thousands.

\subsubsection{Spin locks and busy waiting}

With the success of spin locks and busy waiting in the global reduction code
GMCFs per-thread receiving FIFO queues were looked at as another potential
source of performance improvement. Model threads will regularly sleep, waiting
for packets to arrive. This reduces CPU usage but increases the latency between
a packet arriving and a thread being awake and able to process it. MPI processes
do not sleep by default for this reason. The FIFO queues in GMCF were currently
managed by mutual exclusion locks and condition variables. These were replaced
with spin locks and busy waiting. This reduced runtimes by around 20\%, a
significant improvement.

This change is the reason behind the lack of 63 and 64 thread runs in the GMCF
evaluation. This is due to two non-model threads that are created by the GMCF
framework. These two threads currently use the same code for their FIFO queues.
As a result these two threads needlessly spin and waste the computational
resources of two cores. This is a temporary implementation detail as future work
will include creating a separate FIFO queue implementation for these threads to
allow them to sleep again.

\subsubsection{Thread Pinning}

POSIX threads, by default, are free to be mapped to any available CPU core by
the kernel. This can help kernels to even load over all of the CPUs but is not
ideal from a cache perspective. Each time a thread migrates to a different CPU
socket, the cache built up on that socket's cache is wasted. This can have a
significantly negative impact on performance.

To help combat this, GMCF threads were allocated a single core each. This is
identical to the behaviour of MPI's by-core mapping which helped to improve
performance for the MPI version of the LES. The thread pinning change in GMCF is
responsible for the slight degradation in performance at small numbers of
threads as shown in Figure~\ref{fig:gmcfbeforeandafter} but improved performance
at higher numbers of threads by around 17\%. Again, since the assumption is that
model simulations are the only running programs on a machine and thus use all
CPU cores available, this is a worthwhile change.

\subsubsection{Miscellaneous Changes}

In addition to lower level changes, framework-specific changes were also made.
For example, the original GMCF had one queue for each thread for each type of
packet. Since a thread may be looking for a packet from a specific thread, the
queues had to be iterated over to find a packet from the requested source
thread. To prevent this, each thread had a map of queues for each packet type
and each source thread. This meant that, to find a specific packet, only one
queue had to be looked at and the required packet is guaranteed to either be at
the head of the queue. This map of FIFOs helped speed up the code that waited
for and found packets resulting in an overall performance improvement of nearly
20\%.

\subsection{Fortran and C++ interoperability issue}
\label{sec:fortrancppinteroperability}

That weird Fortran/C++ stack array above a certain size sharing thing.
